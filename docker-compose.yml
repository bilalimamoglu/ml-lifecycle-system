version: '3'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow:
    build: ./airflow
    restart: always
    depends_on:
      - postgres
      - mlflow-server
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
      DOCKER_GID: ${DOCKER_GID}  # Pass the Docker GID from .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db upgrade && 
               airflow users create --username myuser --password mypassword --firstname Firstname --lastname Lastname --role Admin --email example@example.com && 
               airflow scheduler & airflow webserver"

  mlflow-server:
    image: mlflow-server
    build:
      context: ./mlflow
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=https://s3.amazonaws.com
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts-bilalimg/
    volumes:
      - mlflow_db:/mlflow_db

  ml-lifecycle-training:
    build:
      context: ./training
      dockerfile: Dockerfile
    image: ml-lifecycle-training
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000
    depends_on:
      - mlflow-server
    volumes:
      - $HOME/.aws:/root/.aws:ro  # Mount the .aws directory to /root/.aws

volumes:
  postgres_data:
  mlflow_db:
